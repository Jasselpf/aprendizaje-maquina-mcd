# Mas sobre problemas de clasificacion

```{r, include = FALSE}
library(tidyverse)
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
theme_set(theme_minimal())
```

En esta parte presentamos tecnicas adicionales para evaluar el 
desempeÒo de un modelo. En la parte anterior vimos que

- La **devianza** es una buena medida para ajustar y evaluar el desempe√±o de un modelo y 
comparar modelos, y utiliza las probabilidades de clase. Sin embargo, es una medida de dificil de interpretar en cuanto 
a los errores que podemos esperar del modelo.

- Por otro lado, la **tasa de clasificaci√≥n incorrecta** puede
usarse para evaluar el desempe√±o de un clasificador 
(incluyendo uno derivado de probabilidades de clase), puede interpretarse
con facilidad,
pero se queda corta en muchas aplicaciones. Una deficiencia grande
de esta medida es que, contrario al problema de regresi√≥n, hay errores
de clasificaci√≥n que son cualitativamente diferentes.

#### Ejemplo {-}
- Por ejemplo, diagnosticar a alguien con una enfermedad cuando no la tiene
tiene consecuencias distintas a diagnosticar como libre de enfermedad a alguien
que la tiene. Estas consecuencias dependen de c√≥mo son son los tratamientos consecuentes, de y qu√© tan peligrosa es la enfermedad.

- Cuando usamos un buscador como Google, es cualitativamente diferente que el
buscador omita resultados relevantes a que nos presente resultados irrelevantes.

- ¬øOtros ejemplos?

En general, los costos de los distintos errores son distintos, y en muchos
problemas quis√©ramos entenderlos y controlarlos individualmente. Aunque en teor√≠a
podr√≠amos asignar costos a los errores y definir una funci√≥n de p√©rdida apropiada,
en la pr√°ctica esto muchas veces no es tan f√°cil o deseable. Podemos, sin embargo,
reportar el tipo de errores que ocurren

```{block2, type='comentario'}
**Matriz de confusi√≥n**.
Sea $\hat{G}$ un clasificador. La matriz de confusi√≥n $C$ de $\hat{G}$ est√° 
dada por $C_{i,j} =$ n√∫mero de casos de la clase verdadera $j$ que son clasificados como clase $i$
 por el clasificador
```

#### Ejemplo {-} 

En un ejemplo de tres clases, podr√≠amos obtener la matriz de confusi√≥n:

```{r, echo=FALSE}
tabla_1 <- data.frame(A=c(50,20,20), B=c(2,105,10), C=c(0,10,30))
rownames(tabla_1) <- c('A.pred', 'B.pred', 'C.pred')
tabla_1 <- as.table(as.matrix(tabla_1))
knitr::kable(tabla_1)
```

Esto quiere decir que de 90 casos de clase $A$, s√≥lo clasificamos
a 50 en la clase correcta, de 117 casos de clase $B$, acertamos en 105, etc√©tera.
Podemos ver esta tabla de distintas formas, por ejemplo, usando porcentajes
por columna, nos dice c√≥mo se distribuyen los casos de cada clase:

```{r}
knitr::kable(round(prop.table(tabla_1, 2),2))
```

Mientras que una tabla de porcentajes por rengl√≥n nos muestra
qu√© pasa cada vez que hacemos una predicci√≥n dada:

```{r}
knitr::kable(round(prop.table(tabla_1, 1),2))
```

Ahora pensemos c√≥mo podr√≠a sernos de utilidad esta tabla. Discute

- El clasificador fuera uno de severidad de emergencias en un hospital,
donde A=requiere atenci√≥n inmediata B=urgente C=puede posponerse.

- El clasificador fuera de tipos de cliente de un negocio. Por ejemplo,
A = cliente de gasto alto, B=cliente medio, C=abandonador. Imag√≠nate
que tiene un costo intentar conservar a un abandonador, y hay una inversi√≥n
alta para tratar a los clientes A.

La tasa de incorrectos es la misma en los dos ejemplos, pero la adecuaci√≥n
del clasificador es muy diferente.

## An√°lisis de error para clasificadores binarios

Cuando la variable a predecir es binaria (dos clases), podemos
etiquetar una clase como *positiva* y otra como *negativa*. En el fondo
no importa c√≥mo catalogemos cada clase, pero para problemas particulares
una asignaci√≥n puede ser m√°s natural. Por ejemplo, en diagn√≥stico de 
enfermedades, positivo=tiene la enfermedad, en an√°lisis de cr√©dito,
positivo=cae en impago, en sistemas de recomendacion, positivo = le gusta
el producto X, en recuperaci√≥n de textos, positivo=el documento es relevante a la
b√∫squeda, etc.


```{block2, type='comentario'}
Hay dos tipos de errores en un clasificador binario (positivo - negativo):

- Falsos positivos (fp): clasificar como positivo a un caso negativo.
- Falsos negativos (fn): clasificar como negativo a un caso positivo.

A los casos clasificados correctamente les llamamos positivos verdaderos (pv)
y negativos verdaderos (nv).
```

La matriz de confusion es entonces


```{r, warning=FALSE, message=FALSE}
tabla <- data_frame(' ' = c('positivo.pred','negativo.pred','total'),
                    'positivo'=c('vp','fn','pos'),
                    'negativo'=c('fp','nv','neg'),
                    'total' = c('pred.pos','pred.neg',''))
knitr::kable(tabla)
```



N√≥tese que un clasificador bueno, en general, es uno
que tiene la mayor parte de los casos en la diagonal de la matriz
de confusi√≥n.

Podemos estudiar a nuestro clasificador en t√©rminos de las proporciones de casos que caen en cada celda, que dependen del desempe√±o del clasificador en cuanto a casos positivos y negativos. La nomenclatura puede ser
confusa, pues en distintas √°reas se usan distintos nombres para estas proporciones:

- Tasa de falsos positivos
$$\frac{fp}{fp+nv}=\frac{fp}{neg}$$

- Tasa de falsos negativos
$$\frac{fn}{pv+fn}=\frac{fn}{pos}$$

- Especificidad
$$\frac{vn}{fp+vn}=\frac{vn}{neg}$$

- Sensibilidad o Recall
$$\frac{vp}{vp+fn}=\frac{vp}{pos}$$ 


Y tambi√©n otras que tienen como base las predicciones:

- Valor predictivo positivo o Precisi√≥n
$$\frac{vp}{vp+fp}=\frac{vp}{pred.pos}$$

- Valor predictivo negativo
$$\frac{vn}{fn+vn}=\frac{vn}{pred.neg}$$


Dependiendo de el tema y el objetivo hay medidas m√°s naturales que otras:

- En estad√≠stica muchas veces se usa sensibilidad (recall) y especificidad (proporci√≥n de positivos que detectamos y proporci√≥n de negativos que descartamos). Por ejemplo, si se tratara
de una prueba para detectar riesgo de una enfermedad, sensibilidad nos dice qu√© 
porcentaje de los casos riesgosos estamos capturando (sensibilidad), y especificidad nos dice qu√© tan bien excluimos a los casos no riesgosos (especificidad). Estas dos medidas muestran
directamente como el clasificador discrimina entre positivos y entre negativos.

- En b√∫squeda y recuperaci√≥n de documentos o imagenes, o detecci√≥n de fraude ( donde positivo = el documento es relevante / la transacci√≥n es fraudulenta y negativo = el documento no es relevante / transacci√≥n normal), se usa m√°s comunmente precisi√≥n y recall. Esto es porque nos interesa
saber de todos los resultados con predicci√≥n positiva (documentos o imagenes recuperadas), 
qu√© porcentaje son relevantes (precisi√≥n), y tambi√©n, 
de todos los documentos relevantes (positivos), cu√°les son recuperados (recall).

Pero tiende a considerarse que  especificidad o tasas de falsos positivos es menos
√∫til, pues estas
son cantidades dependen de una gran cantidad de documentos o transacciones 
irrelevantes, y tienden a ser cercanas a 1 para cualquier clasificador razonable (¬øpor qu√©?).



```{block2, type='comentario'}
Cada clasificador tiene un balance distinto especificidad-sensibliidad. Muchas veces no escogemos clasificadores por la tasa
de incorrectos solamente, sino que intentamos buscar un balance adecuado entre el comportamiento de clasificaci√≥n para positivos y para negativos.
```


### Medidas resumen de desempe√±o {-}

La primera medida resumen que vimos es el error de clasificaci√≥n, que no toma en
cuenta el tipo de errores:

- **Tasa de clasificaci√≥n incorrecta**
$$\frac{fn+fv}{neg+pos}$$
Y existen otras medidas que intentan resumir los dos tipos de errores de distinta manera,
como

- **Medida F** (media arm√≥nica de precisi√≥n y recall)
$$2\frac{precision \cdot recall}{precision +  recall}$$
Se usa la la media arm√≥nica que penaliza m√°s fuertemente desempe√±o malo en
alguna de nuestras dos medidas (precisi√≥n y recall) que el promedio arm√≥nico.



#### Ejemplo {-}
Si precision = 0.01 (muy malo) y recall = 1 (excelente), o recall=0.01 y precisi√≥n = 1 (excelente),
la media usual considera igual de buenos estos dos clasificadores. A su vez, estos
dos se califican similar a un clasificador con precision = 0.5 y recall = 0.5. 
Sin embargo, la media arm√≥nica (F) da un score mucho m√°s bajo a los primeros dos
clasificadores:
```{r}
media_armonica <- function(x){
    1/mean(1/x)
}
media_armonica(c(0.01, 1))
media_armonica(c(0.5, 0.5))
```

- **AUC** (area bajo la curva ROC) que veremos m√°s adelante.

### Interpetaci√≥n de res√∫menes de desempe√±o y tasas base {-}

Cuando consideramos las tasas de desempe√±o de un clasificador debemos
comparar con lo que suceder√≠a si no us√°ramos el clasificador (si no us√°ramos datos). 
Una manera de hacer esto consiste en utilizar el *clasificador base*:

- Clasificar todo como negativo (o positivo)

#### Ejemplo {-}
Consideramos un problema donde tenemos 20\% de positivos y 80\% de negativos en una
poblaci√≥n. ¬øCu√°l es el desempe√±o de estos clasificadores base?
Podemos usar la ecuaci√≥n 

$$P(acertar) = P(acertar|pos)P(pos) + P(acertar|neg)P(neg)$$
que en este caso es:

$$P(acertar) = 0.2P(acertar|pos) + 0.8 P(acertar|neg)$$


- Si clasificamos siempre a positivo, la tasa de correctos ser√° de 20\%, pues
$P(acertar|pos) = 1$ y $P(acertar|neg)= 0$
- Si clasificamos siempre a negativo, la tasa de correctos ser√° de 80\%, pues
$P(acertar|pos) = 0$ y $P(acertar|neg)= 1$

En este ejemplo, si tuvi√©ramos un clasificador 
con una tasa de correctos de $75\%$, querr√≠a decir que no logramos mucho
en el sentido de la certeza de nuestra predicci√≥n.

---

```{block2, type="comentario"}
Comparamos la tasa de correctos de clasificadores contra la tasa base
$\max\{p_{pos}, 1- p_{pos}\}$, donde $p_{pos}$ es la tasa de positivos en nuestros
datos. 
```

T√≠picamente esperamos superar esta tasa de correctos base. Nota: cuando los
costos de los distintos errores son muy diferentes, existen otras medidas m√°s apropiadas


#### Ejercicio {-}
Calcular la matriz de confusi√≥n (sobre la muestra de prueba) para el
clasificador log√≠stico de diabetes en t√©rminos de glucosa. Calcula 
adicionalmente con la muestra de prueba sus valores de especificidad y sensibilidad, y precisi√≥n y recall. 

```{r, warnings=FALSE, messages=FALSE}
diabetes_ent <- as_data_frame(MASS::Pima.tr)
diabetes_pr <- as_data_frame(MASS::Pima.te)
mod_1 <- glm(type ~ glu, data = diabetes_ent, family = 'binomial')
preds_prueba <- predict(mod_1, newdata = diabetes_pr, type ='response')
# rellena esta linea en t√©rminos de preds_prueba
# clase_pred <- 
# table(clase_pred)
# ahora calcula la matriz de confusi√≥n
# table()
# Usando esta tabla encuentra 
# tasa de incorrectos especificidad y sensibilidad, precisi√≥n y recall.
```


### Puntos de corte para un clasificador binario

¬øQu√© sucede cuando el perfil de sensibilidad y especificidad de un 
clasificador binario no es apropiado para nuestros fines?
Recordemos que una vez que hemos estimado con $\hat{p}_1(x)$, nuestra regla de clasificaci√≥n es:

1. Predecir positivo si $\hat{p}_1(x) > 0.5$, 
2. Predecir negativo si $\hat{p}_1(x) \leq 0.5.$

Esto sugiere una regla alternativa:

Para $0 < d < 1$, podemos utilizar nuestras estimaciones $\hat{p}_1(x)$ para construir un clasificador alternativo poniendo:

1. Predecir positivo si $\hat{p}_1(x) > d$, 
2. Predecir negativo si $\hat{p}_1(x) \leq d$.


Distintos valores de $d$ dan distintos perfiles de sensibilidad-especificidad para una misma estimaci√≥n de las probabilidades condicionales de clase:
Para minimizar la tasa de incorrectos conviene poner $d = 0.5$. Sin embargo, es com√∫n que este no es el √∫nico fin de un clasificador bueno (pensar en ejemplo de fraude).

- Cuando incrementamos d, quiere decir que exigimos estar m√°s seguros de que un caso es positivo para clasificarlo como positivo. Eso quiere decir que la especifidad va a ser m√°s grande (entre
los negativos verdaderos va a haber menos falsos positivos). Sin embargo, la sensibilidad va a ser m√°s chica pues captamos menos de los verdaderos positivos.

#### Ejemplo {-}
Por ejemplo, si en el caso de diabetes incrementamos el punto de corte a 0.7:
```{r}
table(preds_prueba > 0.7, diabetes_pr$type)
tab <- prop.table(table(preds_prueba > 0.7, diabetes_pr$type), 2)
tab
```

La especificidad ahora `r round(tab[1,1],2)` , muy alta (descartamos muy bien casos negativos), pero la sensibilidad se deteriora a `r round(tab[2,2],2)`


- Cuando hacemos m√°s chico d, entonces exigimos estar m√°s seguros de que un caso es negativo para clasificarlo como negativo. Esto aumenta la sensibilidad, pero la especificidad baja.
Por ejemplo, si en el caso de diabetes ponemos el punto de corte en 0.3:
```{r}
table(preds_prueba > 0.3, diabetes_pr$type)
tab <- prop.table(table(preds_prueba > 0.3, diabetes_pr$type),2)
tab
```

#### Ejemplo {-}

Podemos tener una intuici√≥n de c√≥mo cambian las tasas de error dependiendo
de donde cortamos mostrando la tabla ordenada por probabilidades estimadas
(incluimos tambi√©n las covariables para entender qu√© variables est√°n m√°s
correlacionadas con las probabilidades):
```{r, warnings=FALSE,message=FALSE}
library(tabplot)
mod_1 <- glm(type ~ glu, diabetes_ent, family = 'binomial')
diabetes_pr$probs_prueba_1 <- predict(mod_1, newdata = diabetes_pr,
                                      type = "response") 
head(arrange(diabetes_pr, desc(probs_prueba_1)))
tableplot(diabetes_pr, sortCol = probs_prueba_1)
```



La columna de probabilidad de la derecha nos dice en qu√© valores
podemos cortar para obtener distintos clasificadores. N√≥tese que
si cortamos m√°s arriba, se nos escapan m√°s positivos verdaderos
que clasificamos como negativos, pero clasificamos a m√°s
negativos verdaderos como negativos. Lo opuesto ocurre 
cuando cortamos m√°s abajo.


### Espacio ROC de clasificadores

Podemos visualizar el desempe√±o de cada uno de estos clasificadores construidos
con puntos de corte
mape√°ndolos a las coordenadas de tasa de falsos positivos
(1-especificidad) y sensibilidad:

```{r, fig.width = 5, fig.asp =0.9}
clasif_1 <- data.frame(
  corte = c('0.3','0.5','0.7','perfecto','azar'),
  tasa_falsos_pos=c(0.24,0.08,0.02,0,0.7),
  sensibilidad =c(0.66, 0.46,0.19,1,0.7))
ggplot(clasif_1, aes(x=tasa_falsos_pos, y=sensibilidad,
  label=corte)) + geom_point() + 
  geom_abline(intercept=0, slope=1) +
  xlim(c(0,1)) +ylim(c(0,1)) + geom_text(hjust=-0.3, col='red')+
  xlab('1-especificidad (tasa falsos pos)')

```



1. N√≥tese que agregamos otros dos clasificadores, uno perfecto, que tiene tasa de falsos positivos igual a 0 y sensibilidad igual a 1.
2. En esta gr√°fica, un clasificador $G_2$ que est√° arriba a la izquierda de $G_1$
domina a $G_1$, pues tiene mejor especificidad y mejor sensibilidad. Entre los clasificadores 0.3, 0.5 y 0.7 de la gr√°fica, no hay ninguno que domine a otro.
3. Todos los clasificadores en la diagonal son equivalentes a un clasificador al azar. ¬øPor qu√©? La raz√≥n es que si cada vez que vemos un nuevo caso lo clasificamos como positivo con probabilidad $p$ fija y arbitraria. Esto implica que cuando veamos un caso positivo, la probabilidad de ‚Äôatinarle‚Äô es de p (sensibilidad), y cuando vemos un negativo, la probabilidad de equivocarnos tambi√©n es de 1-p (tasa de falsos positivos), por lo que
la espcificidad es p tambi√©n. De modo que este clasificador al azar est√° en la diagonal.
4. ¬øQu√© podemos decir acerca de clasificadores que caen por debajo de la diagonal? Estos son clasificadores particularmente malos, pues existen clasificadores con mejor especificidad y/o sensibilidad que son clasificadores al azar! Sin embargo, se puede construir un mejor clasificador volteando las predicciones, lo que cambia sensibilidad por tasa de falsos positivos.
5. ¬øCu√°l de los tres clasificadores es el mejor? En t√©rminos de la tasa de incorrectos, el de corte 0.5. Sin embargo, para otros prop√≥sitos puede ser razonable escoger alguno de los otros.

## Perfil de un clasificador binario y curvas ROC

En lugar de examinar cada punto de corte por separado, podemos hacer el an√°lisis de todos los posibles puntos de corte mediante la curva ROC (receiver operating characteristic, de ingenier√≠a).


```{block2, type='comentario'}
 Para un problema de clasificaci√≥n binaria, dadas estimaciones $\hat{p}(x)$, 
 la curva ROC grafica todos los pares de (1-especificidad, sensibilidad) para cada posible punto de corte $\hat{p}(x) > d$.
 
```

 
Vamos a graficar todos los pares (1-especificidad, sensibilidad)
para cada punto de corte $d$ de estas probabilidades.

```{r, message=FALSE, warning=FALSE}
library(ROCR)
pred_rocr <- prediction(diabetes_pr$probs_prueba_1, diabetes_pr$type) 
perf <- performance(pred_rocr, measure = "sens", x.measure = "fpr") 
graf_roc_1 <- data_frame(tfp = perf@x.values[[1]], sens = perf@y.values[[1]], 
                       d = perf@alpha.values[[1]])

ggplot(graf_roc_1, aes(x = tfp, y = sens, colour=d)) + geom_point() +
  xlab('1-especificidad') + ylab('Sensibilidad') 
```

En esta gr√°fica podemos ver todos los clasificadores posibles basados
en las probabilidades de clase. Podemos usar estas curvas como evaluaci√≥n
de nuestros clasificadores, dejando para m√°s tarde la selecci√≥n del punto de
corte, si esto es necesario (por ejemplo, dependiendo de los costos de cada
tipo de error).

Tambi√©n podemos definir una medida resumen del desempe√±o de un clasificador seg√∫n
esta curva:

```{block2, type='comentario'}
La medida AUC (area under the curve) para un clasificador es el √°rea 
bajo la curva generada por los pares sensibilidad-especificidad de la curva ROC.
```

```{r}
auc_1 <- performance(pred_rocr, measure = 'auc')@y.values
auc_1[[1]]
```


Tambi√©n es √∫til para comparar modelos. Consideremos el modelo de los datos
de diabetes que incluyen todas las variables:
```{r, warnings=FALSE,message=FALSE}
mod_2 <- glm(type ~ ., diabetes_ent, family = 'binomial')
diabetes_pr$probs_prueba_2 <- predict(mod_2, newdata = diabetes_pr,
                                      type = "response") 
head(arrange(diabetes_pr, desc(probs_prueba_2)))
tableplot(diabetes_pr, sortCol = probs_prueba_2)
```


Y graficamos juntas:

```{r}
library(ROCR)
pred_rocr <- prediction(diabetes_pr$probs_prueba_2, diabetes_pr$type) 
perf <- performance(pred_rocr, measure = "sens", x.measure = "fpr") 
auc_2 <- performance(pred_rocr, measure = "auc")@y.values
graf_roc_2 <- data_frame(tfp = perf@x.values[[1]], sens = perf@y.values[[1]], 
                       d = perf@alpha.values[[1]])

graf_roc_2$modelo <- 'Todas las variables'
graf_roc_1$modelo <- 'Solo glucosa'
graf_roc <- bind_rows(graf_roc_1, graf_roc_2)

ggplot(graf_roc, aes(x = tfp, y = sens, colour = modelo)) + geom_point() +
  xlab('1-especificidad') + ylab('Sensibilidad') 
```

Comparaci√≥n auc:

```{r}
auc_1
auc_2
```

En este ejemplo, vemos que casi no importa que perfil de especificidad y sensibilidad busquemos: el clasificador que usa todas las variables
domina casi siempre al clasificador que s√≥lo utiliza las variables de glucosa. 
La raz√≥n es que para cualquier punto de corte (con sensibilidad menor a 0.4) en el clasificador de una variable, existe otro clasificador en la curva roja (todas las variable), que domina al primero. La excepci√≥n es para clasificadores de valores de sensibilidad baja, con tasas de falsos positivos muy chicas: 
en este caso, el modelo de una variable puede ser ligeramente superior.

## Regresi√≥n log√≠stica para problemas de m√°s de 2 clases

Consideramos ahora un problema con m√°s de dos clases, de manera que $G ‚àà {1,2,...,K}$
($K$ clases), y tenemos $X = (X1 ...,Xp)$ entradas.
¬øC√≥mo generalizar el modelo de regresi√≥n log√≠stica a este problema?
Una estrategia es la de uno contra todos:

En clasificaci√≥n *uno contra todos*, hacemos

1. Para cada clase $g\in\{1,\ldots,K\}$ entrenamos un modelo de regresi√≥n
log√≠stica (binaria) $\hat{p}^{(g)}(x)$, tomando como positivos a los casos de 1
clase $g$, y como negativos a todo el resto. Esto lo hacemos como en las secciones anteriores, y de manera independiente para cada clase.

2. Para clasificar un nuevo caso $x$, 
calculamos 
$$\hat{p}^{(1)}, \hat{p}^{(2)},\ldots, \hat{p}^{(K)}$$

y clasificamos a la clase de m√°xima probabilidad
$$\hat{G}(x) = \arg\max_g \hat{p}^{(g)}(x)$$
N√≥tese que no hay ninguna garant√≠a de que las probabilidades de clase
sumen 1, pues se trata de estimaciones independientes de cada clase. En este sentido, produce estimaciones que en realidad no satisfacen las propiedades del modelo de probabilidad establecido - aunque pueden normalizarse. Sin embargo, esta estrategia es simple y en 
muchos casos funciona bien.

### Regresi√≥n log√≠stica multinomial

Si queremos obtener estimaciones de las probabilidades de clase que sumen uno, entonces tenemos que contruir las estimaciones de cada clase de clase de manera conjunta.
Como vimos antes, tenemos que estimar, para cada $x$ y $g\in\{1,\ldots, K\}$,
las probabilidades condicionales de clase:
$$p_g(x) = P(G = g|X = x).$$

Consideremos primero c√≥mo funciona el modelo de regresi√≥n log√≠stica (2 clases)

Tenemos que
$$p_1(x) = h(\beta_0 + \beta_1x_1 + \ldots + \beta_p x_p) =
\exp(\beta_0 + \beta_1x_1 + \ldots + \beta_p x_p)/Z
$$
y
$$p_2 (x) = 1/Z$$
donde $Z = 1 + \exp(\beta_0 + \beta_1x_1 + \ldots + \beta_p x_p)$.

Podemos generalizar para m√°s de 2 clases usando una idea similar. Cada clase
tiene su juego de coeficientes:

$$p_1(x) =  \exp(\beta_{0,1} + \beta_{1,1}x_1 + \ldots + \beta_{p,1} x_p)/Z$$

$$p_2(x) =  \exp(\beta_{0,2} + \beta_{1,2}x_2 + \ldots + \beta_{p.2} x_p)/Z$$
hasta
$$p_{K-1}(x) =  \exp(\beta_{0,{K-1}} + \beta_{1,{K-1}}x_2 + \ldots + \beta_{p,{K-1}} x_p)/Z$$
y 
$$p_K(x) = 1/Z$$

En este caso, para que las probabilidades sumen 1, necesitamos que
$$Z = 1 + \sum_{j=1}^{K-1}\exp(\beta_0^j + \beta_1^jx_2 + \ldots + \beta_p^j x_p)$$

Para ajustar coeficientes, usamos el mismo criterio de devianza de entrenamiento.
Buscamos minimizar:
$$D(\beta)=‚àí2 \sum_{i=1}^N p_{g^{(i)}}(x^{(i)}),$$
Donde $\beta$ contiene todos los coeficientes organizados en un vector
de tama√±o $(p+1)(K+1)$:
$$\beta = ( \beta_0^1, \beta_1^1, \ldots , \beta_p^1,  \beta_0^2, \beta_1^2, \ldots , \beta_p^2, \ldots \beta_0^{K-1}, \beta_1^{K-1}, \ldots , \beta_p^{K-1} )$$

Y ahora podemos usar alg√∫n m√©todo n√∫merico para minimizar la devianza (por ejemplo,
descenso en gradiente).  Cuando
es muy importante tener  probabilidades bien calibradas, el enfoque multinomial
es m√°s apropiado, pero muchas veces, especialmente si s√≥lo nos interesa clasificar, los
dos m√©todos dan resultados similares.

### Interpretaci√≥n de coeficientes

Los coeficientes mostrados en la parametrizaci√≥n de arriba se intrepretan
m√°s f√°cilmente como comparaciones de la clase $j$ contra la clase $K$, pues

$$\log\left (\frac{p_g(x)}{p_K(x)}\right ) = \beta_{0,{g}} + \beta_{1,{g}}x_2 + \ldots + \beta_{p,{g}} x_p$$

Para comparar la clase $j$ con la clase $k$ notamos que

$$\log\left (\frac{p_j(x)}{p_k(x)}\right ) = 
(\beta_{0,{j}}- \beta_{0,{k}}) + (\beta_{1,{j}}-\beta_{1,{k}} )x_2 + \ldots + (\beta_{p,{j}} -\beta_{p,{k}})  x_p$$

As√≠ que s√≥lo hace falta restar los coeficientes. N√≥tese adicionalmente
que en la parametrizaci√≥n, podemos pensar que

$$\beta_{0,K} = \beta_{1,K} = \cdots = \beta_{p,K} = 0$$ 


### Ejemplo: Clasificaci√≥n de d√≠gitos con regresi√≥n multinomial

```{r}
digitos_entrena <- read_csv('datos/zip-train.csv')
digitos_prueba <- read_csv('datos/zip-test.csv')
names(digitos_entrena)[1] <- 'digito'
names(digitos_entrena)[2:257] <- paste0('pixel_', 1:256)
names(digitos_prueba)[1] <- 'digito'
names(digitos_prueba)[2:257] <- paste0('pixel_', 1:256)
```

En este ejemplo, usamos la funci√≥n *multinom* de *nnet*, que usa
BFGS para hacer la optimizaci√≥n:
```{r}
library(nnet)
mod_mult <- multinom(digito ~ ., data = digitos_entrena, MaxNWt=100000, maxit = 20)
```

Checamos para diagn√≥stico la matriz de confusi√≥n **de entrenamiento**.

```{r}
table(predict(mod_mult), digitos_entrena$digito)
```


Ahora validamos con la muestra de prueba y calculamos error de clasificaci√≥n:
```{r}
confusion_prueba <- table(predict(mod_mult, newdata = digitos_prueba), digitos_prueba$digito)
confusion_prueba
sum(diag(confusion_prueba))/sum(confusion_prueba)
round(prop.table(confusion_prueba, 2),2)
```

El resultado no es muy bueno. Veremos m√°s adelante mejores m√©todos para 
este problema. ¬øPodemos interpretar el modelo?

Una idea es tomar los coeficientes y graficarlos seg√∫n la estructura de
las im√°genes:

```{r}
coefs <- coef(mod_mult)
coefs_reng <- coefs[1, , drop =FALSE]
coefs <- rbind(coefs_reng, coefs)
coefs[1 , ] <- 0
dim(coefs)
beta_df <- coefs[,-1] %>% as.data.frame %>% 
  mutate(digito = 0:(nrow(coefs)-1)) %>%
  gather(pixel, valor, contains('pixel')) %>%
  separate(pixel, into = c('str','pixel_no'), sep='_') %>%
  mutate(x = (as.integer(pixel_no)-1) %% 16, y = -((as.integer(pixel_no)-1) %/% 16))
head(beta_df)
```

Podemos cruzar la tabla con s√≠ misma para hacer comparaciones de c√≥mo discrimina
el modelo entre cada par de d√≠gitos:

```{r}
tab_coef <- beta_df %>% select(digito, x, y, valor)
tab_coef_1 <- tab_coef
names(tab_coef_1) <- c('digito_1','x','y','valor_1')
tab_cruzada <- full_join(tab_coef_1, tab_coef) %>% mutate(dif = valor_1 - valor)
tab_cruzada <- tab_cruzada %>% group_by(digito, digito_1) %>% 
  mutate(dif_s = (dif - mean(dif))/sd(dif)) %>%
  mutate(dif_p = pmin(pmax(dif_s, -2), 2))
```

```{r}
ggplot(tab_cruzada, aes(x=x, y=y)) + geom_tile(aes(fill = dif_p)) + 
  facet_grid(digito_1~digito) + scale_fill_distiller(palette = "Spectral")
```



### Discusi√≥n {-}

N√≥tese que no corrimos el modelo hasta convergencia. Vamos a hacerlo ahora:


```{r}
mod_mult <- multinom(digito ~ ., data = digitos_entrena, MaxNWt=100000, maxit = 500)
```


```{r, cache = TRUE}
confusion_prueba <- table(predict(mod_mult, newdata = digitos_prueba), digitos_prueba$digito)
confusion_prueba
sum(diag(confusion_prueba))/sum(confusion_prueba)
round(prop.table(confusion_prueba, 2),2)
```

Y nota que el error es m√°s grande que cuando nos detuvimos antes. Discute en clase:

- Grafica los coeficientes para este segundo modelo
- ¬øEn cu√°l de los dos modelos es m√°s f√°cil interpretar los coeficientes? ¬øEn cu√°l
es menor el error?
- ¬øCu√°l crees que es el problema de este segundo modelo comparado con el primero? ¬øPor qu√© crees que sucede? ¬øC√≥mo podr√≠amos corregir este problema?

## Descenso en gradiente para regresi√≥n multinomial log√≠stica

Supondremos $K$ clases, numeradas de $0,1,\ldots, K-1$. *OJO*: al aplicar
este c√≥digo debes ser cuidadoso con las etiquetas de clase.

```{r}
pred_multinom <- function(x, beta){
  p <- ncol(x)
  K <- length(beta)/(p+1) + 1
  beta_mat <- matrix(beta, K - 1, p + 1 , byrow = TRUE)
  u_beta <- exp(as.matrix(cbind(1, x)) %*% t(beta_mat))
  Z <- 1 + apply(u_beta, 1, sum)
  p_beta <- cbind(u_beta, 1)/Z
  as.matrix(p_beta)
}

devianza_calc <- function(x, y){
  dev_fun <- function(beta){
    p_beta <- pred_multinom(x, beta)
    p <- sapply(1:nrow(x), function(i) p_beta[i, y[i]+1])
   -2*sum(log(p))
  }
  dev_fun
}

grad_calc <- function(x_ent, y_ent){
  p <- ncol(x_ent)
  K <- length(unique(y_ent)) 
  y_fact <- factor(y_ent) 
  # matriz de indicadoras de clase
  y_dummy <-  model.matrix(~-1 + y_fact)
  salida_grad <- function(beta){
    p_beta <-  pred_multinom(x_ent, beta)
    e_mat <-  (y_dummy  - p_beta)[, -K]
    grad_out <- -2*(t(cbind(1,x_ent)) %*% e_mat)
    as.numeric(grad_out)
  }
  salida_grad
}
descenso <- function(n, z_0, eta, h_deriv, dev_fun){
  z <- matrix(0,n, length(z_0))
  z[1, ] <- z_0
  for(i in 1:(n-1)){
    z[i+1, ] <- z[i, ] - eta * h_deriv(z[i, ])
    if(i %% 100 == 0){
      print(paste0(i, ' Devianza: ', dev_fun(z[i+1, ])))
    }
  }
  z
}
```


```{r}
x_ent <- digitos_entrena %>% select(contains('pixel')) %>% as.matrix
y_ent <- digitos_entrena$digito
x_ent_s <- scale(x_ent)
medias <- attr(x_ent_s, 'scaled:center')
sd <- attr(x_ent_s, 'scaled:scale')
x_pr <- digitos_prueba %>% select(contains('pixel')) %>% as.matrix
y_pr <- digitos_prueba$digito
# inicializamos coeficientes al azar
beta <- runif(257*9)
dev_ent <- devianza_calc(x_ent_s, y_ent)
grad <- grad_calc(x_ent_s, y_ent)
dev_ent(beta)
```

Hacemos algunas revisiones del gradiente:

```{r}
beta_2 <- beta 
epsilon <- 0.00001
beta_2[1000] <- beta[1000] + epsilon

(dev_ent(beta_2) - dev_ent(beta))/epsilon
```

```{r}
grad(beta)[1000]
```

Ya ahora podemos hacer descenso:

```{r, cache = TRUE}
iteraciones <- descenso(2000, rep(0, 257*9), eta=0.001, 
                        h_deriv = grad, dev_fun = dev_ent)

x_pr_s <- scale(x_pr, center = medias, scale = sd)
probas <- pred_multinom(x_pr_s, iteraciones[2000,])
clase <- apply(probas, 1, which.max)
table(clase - 1, y_pr )
1 - mean(clase-1 != y_pr)
```



#### Tarea 4 {-}
Ver *tareas/tarea_4.Rmd*.

